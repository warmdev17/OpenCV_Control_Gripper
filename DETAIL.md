# Motion-Controlled Robot Arm Project

### **Ultra-Detailed Task README (For Vibe Coding Team)**

This README is designed so that **any team member can copy their
assigned task into AI and immediately receive runnable code** without
understanding the whole project.

------------------------------------------------------------------------

# â­ Project Summary

A camera captures your hand â†’ software detects landmarks â†’ system
extracts gesture + hand position â†’ mapping logic converts those into
servo angles â†’ Arduino moves a 4â€‘servo robotic arm (base, shoulder,
elbow, gripper).

------------------------------------------------------------------------

# ğŸ§© Project Modules (10 Modules -- Leader does 2)

Each module is **independent** and has **extremely specific
inputs/outputs**, so team members can code individually without
conflicts.

------------------------------------------------------------------------

# 1. camera_reader.py

## ğŸ¯ Goal

Create a module that **opens webcam**, **reads frames continuously**,
and **returns frames in 640Ã—480 resolution**.

## ğŸ“¥ Input

Nothing.

## ğŸ“¤ Output

Function:

``` python
get_frame() -> numpy.ndarray | None
```

## ğŸ”§ Detailed Work Instructions

-   Import OpenCV.
-   Initialize the default camera (ID = 0).
-   Set resolution to 640x480.
-   Write `get_frame()` that:
    -   reads a frame
    -   converts it to RGB
    -   returns the frame
    -   returns `None` if read fails.
-   Ensure FPS â‰¥ 20.
-   Include cleanup function if camera needs to be released.

## ğŸ“ AI Prompt Template

"Write a Python module named `camera_reader.py` that opens webcam, reads
frames in 640Ã—480, returns them as numpy arrays via get_frame(), returns
None on failure, and keeps FPS above 20."

------------------------------------------------------------------------

# 2. hand_detector.py

## ğŸ¯ Goal

Use **Mediapipe Hands** to detect **21 3â€‘D landmarks**.

## ğŸ“¥ Input

`frame` (numpy RGB image)

## ğŸ“¤ Output

List of 21 points:

    [(x,y,z), (x,y,z), ...] or None

## ğŸ”§ Detailed Work Instructions

-   Import mediapipe.
-   Initialize `mp.solutions.hands.Hands` with:
    -   `max_num_hands=1`
    -   `min_detection_confidence=0.7`
    -   `min_tracking_confidence=0.7`
-   Convert input frame to RGB.
-   Run detection.
-   If no hand: return None.
-   If hand detected:
    -   extract 21 landmark points
    -   normalize them to float values between 0--1.

## ğŸ“ AI Prompt Template

"Write a module `hand_detector.py` that takes an RGB frame and returns
list of 21 normalized mediapipe hand landmarks or None."

------------------------------------------------------------------------

# 3. landmark_filter.py

## ğŸ¯ Goal

Smooth the 21 landmark points to remove jitter.

## ğŸ“¥ Input

`raw_landmarks: list[(x,y,z)]`

## ğŸ“¤ Output

`filtered_landmarks: list[(x,y,z)]`

## ğŸ”§ Detailed Work Instructions

-   Implement EMA smoothing:

        smoothed = alpha * new + (1-alpha) * old

-   Use `alpha = 0.35`.

-   Store previous landmark state inside the module.

-   Maintain shape exactly: 21 elements, each 3 values.

-   If previous landmarks do not exist â†’ return input as-is.

## ğŸ“ AI Prompt Template

"Write `landmark_filter.py` that applies EMA smoothing to 21 hand
landmarks using alpha=0.35."

------------------------------------------------------------------------

# 4. gesture_detector.py

## ğŸ¯ Goal

Classify gesture based on finger openness.

## ğŸ“¥ Input

`landmarks_filtered`

## ğŸ“¤ Output

One of:

    "open" | "close" | "pinch" | "idle"

## ğŸ”§ Detailed Work Instructions

-   Use tip positions of fingers vs their base joints.
-   Example rules:
    -   All fingertips far â†’ "open"
    -   All fingertips near palm â†’ "close"
    -   Thumb tip close to index tip â†’ "pinch"
    -   Otherwise â†’ "idle"
-   Must not modify input.

## ğŸ“ AI Prompt Template

"Write `gesture_detector.py` that takes 21 filtered landmarks and
returns gesture string open/close/pinch/idle using fingertip distances."

------------------------------------------------------------------------

# 5. hand_position.py

## ğŸ¯ Goal

Convert normalized coordinates â†’ pixel coordinates in 640Ã—480.

## ğŸ“¥ Input

`landmarks_filtered`

## ğŸ“¤ Output

Tuple:

    (x_pixel, y_pixel)

## ğŸ”§ Detailed Work Instructions

-   Use landmark index 9 (center of palm) **or** combine multiple
    points.

-   Convert by:

        x_pixel = x_norm * 640
        y_pixel = y_norm * 480

-   Return integers.

## ğŸ“ AI Prompt Template

"Write `hand_position.py` that converts normalized palm landmark to
(x_pixel, y_pixel) on a 640Ã—480 frame."

------------------------------------------------------------------------

# 6. pc_sender.py

## ğŸ¯ Goal

Send servo angles to Arduino over Serial.

## ğŸ“¥ Input

Tuple:

    (base, shoulder, elbow, gripper)

## ğŸ“¤ Output

Serial transmission string:

    B:<val>;S:<val>;E:<val>;G:<val>

## ğŸ”§ Detailed Work Instructions

-   Use pyserial.
-   Auto-detect port OR allow fixed port.
-   Reconnect if cable unplugged.
-   Convert angles to int 0--180.
-   Send formatted string.
-   Flush output.

## ğŸ“ AI Prompt Template

"Write `pc_sender.py` that sends servo angles to Arduino using pyserial
in format B:x;S:y;E:z;G:w."

------------------------------------------------------------------------

# 7. ui_debugger.py

## ğŸ¯ Goal

Show debug window with: - video frame\
- drawn landmarks\
- gesture text\
- servo angles\
- FPS

## ğŸ“¥ Input

-   frame\
-   gesture\
-   angles

## ğŸ“¤ Output

-   Debug window (OpenCV imshow)

## ğŸ”§ Detailed Work Instructions

-   Overlay landmarks as small circles.
-   Add text in upper-left corner.
-   Add FPS counter using time.time().
-   Show servo angles clearly.

## ğŸ“ AI Prompt Template

"Write `ui_debugger.py` that draws landmarks, gesture, angles, and FPS
on a frame and shows it via cv2.imshow."

------------------------------------------------------------------------

# 8. servo_control.ino (Arduino)

## ğŸ¯ Goal

Parse serial input and move 4 servos.

## ğŸ“¥ Input

String:

    B:val;S:val;E:val;G:val

## ğŸ“¤ Output

PWM control for 4 servo motors.

## ğŸ”§ Detailed Work Instructions

-   Use `Servo.h`.
-   Create 4 Servo objects: base, shoulder, elbow, gripper.
-   Setup correct PWM pins.
-   In loop:
    -   Read Serial line
    -   Parse values
    -   Constrain 0--180
    -   Write to each servo
-   No delay() except 1--2 ms.

## ğŸ“ AI Prompt Template

"Write Arduino code that reads a serial command B:x;S:y;E:z;G:w and
moves 4 servos accordingly."

------------------------------------------------------------------------

# 9. mapping.py (Leader)

## ğŸ¯ Goal

Convert gesture + (x,y) hand position â†’ 4 servo angles.

## ğŸ“¥ Input

-   gesture\
-   x_pixel\
-   y_pixel

## ğŸ“¤ Output

Tuple of 4 servo angles.

## ğŸ”§ Detailed Work Instructions

### Base

-   Rotate from left--right of frame

        base = map(x, 0â†’640, 20â†’160)

### Shoulder

-   Move based on vertical position

        shoulder = map(y, 0â†’480, 30â†’150)

### Elbow

-   Opposite of shoulder for natural motion

        elbow = 180 - shoulder

### Gripper

-   Based on gesture

        open  â†’ 160Â°
        close â†’ 20Â°
        pinch â†’ 60Â°
        idle  â†’ keep previous

-   Always constrain to 0--180.

## ğŸ“ AI Prompt Template

"Write `mapping.py` that turns gesture + xy pixel into 4 servo angles
using predefined mapping rules."

------------------------------------------------------------------------

# 10. main.py (Leader)

## ğŸ¯ Goal

Integrate all modules into one pipeline.

## ğŸ”§ Detailed Work Instructions

-   Import all 9 modules.
-   Infinite loop:
    1.  frame = get_frame()
    2.  landmarks = detect_hand(frame)
    3.  filtered = filter_landmarks(landmarks)
    4.  gesture = detect_gesture(filtered)
    5.  x,y = calc_hand_position(filtered)
    6.  angles = mapping(gesture, x, y)
    7.  send_to_arduino(angles)
    8.  show_debug(frame, gesture, angles)
-   Maintain â‰¥15 FPS.
-   Handle:
    -   camera missing\
    -   serial disconnected\
    -   no landmarks

## ğŸ“ AI Prompt Template

"Write `main.py` that connects 9 modules into a real-time loop executing
the full robot arm control pipeline."

------------------------------------------------------------------------

# ğŸ‰ Final Notes

-   All modules are **fully independent**.\
-   Every team member can simply paste their section into ChatGPT/AI to
    get code.\
-   Leader handles mapping + main (most difficult).\
-   No need for machine learning training.
